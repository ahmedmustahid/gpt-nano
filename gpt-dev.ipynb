{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841f9654-c058-4ef7-ab49-efb39b64e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f43234-80aa-41de-9bab-cbaba7e2b884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03f20d9-85a0-4884-9c98-0d1808e586f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e79dc-5550-4c77-8e21-cebbb46038e5",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e197cd05-5a9a-4413-a77d-80fb6807ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for i,s in enumerate(chars)}\n",
    "encode = lambda x: [stoi[c] for c in x]\n",
    "decode = lambda x: \"\".join([itos[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd691f00-18ed-497a-8119-6002e0628d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46, 47, 1, 58, 46, 43, 56, 43]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"hi there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7d14274-8013-48f4-bf1d-33eda793836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi there'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([46, 47, 1, 58, 46, 43, 56, 43])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b10f6-19c3-4a2a-b42c-c12252c1f16a",
   "metadata": {},
   "source": [
    "#### Tensorizing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac2a4de-fa10-468b-8eb8-5eed2cbe8e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data.shape\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3294827f-f442-47d8-9857-8355ac8beb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f516737a-8515-49b7-8261-a629a4ca797d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18])--->47\n",
      "tensor([18, 47])--->56\n",
      "tensor([18, 47, 56])--->57\n",
      "tensor([18, 47, 56, 57])--->58\n",
      "tensor([18, 47, 56, 57, 58])--->1\n",
      "tensor([18, 47, 56, 57, 58,  1])--->15\n",
      "tensor([18, 47, 56, 57, 58,  1, 15])--->47\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])--->58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "\n",
    "    print(f\"{context}--->{target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e6f36-4a13-4c87-a1cc-150b7609df6b",
   "metadata": {},
   "source": [
    "this is the time dimension. another dimension is batch dimension. chunks of data processed in gpu independently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ec4b3-b0e2-4a66-a326-9d4e6b5e2235",
   "metadata": {},
   "source": [
    "### Creating batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af4076a4-942e-4b99-92f5-294c6b0374c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets: tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "input: [24]--->target: [43]\n",
      "input: [24, 43]--->target: [58]\n",
      "input: [24, 43, 58]--->target: [5]\n",
      "input: [24, 43, 58, 5]--->target: [57]\n",
      "input: [24, 43, 58, 5, 57]--->target: [1]\n",
      "input: [24, 43, 58, 5, 57, 1]--->target: [46]\n",
      "input: [24, 43, 58, 5, 57, 1, 46]--->target: [43]\n",
      "input: [24, 43, 58, 5, 57, 1, 46, 43]--->target: [39]\n",
      "input: [44]--->target: [53]\n",
      "input: [44, 53]--->target: [56]\n",
      "input: [44, 53, 56]--->target: [1]\n",
      "input: [44, 53, 56, 1]--->target: [58]\n",
      "input: [44, 53, 56, 1, 58]--->target: [46]\n",
      "input: [44, 53, 56, 1, 58, 46]--->target: [39]\n",
      "input: [44, 53, 56, 1, 58, 46, 39]--->target: [58]\n",
      "input: [44, 53, 56, 1, 58, 46, 39, 58]--->target: [1]\n",
      "input: [52]--->target: [58]\n",
      "input: [52, 58]--->target: [1]\n",
      "input: [52, 58, 1]--->target: [58]\n",
      "input: [52, 58, 1, 58]--->target: [46]\n",
      "input: [52, 58, 1, 58, 46]--->target: [39]\n",
      "input: [52, 58, 1, 58, 46, 39]--->target: [58]\n",
      "input: [52, 58, 1, 58, 46, 39, 58]--->target: [1]\n",
      "input: [52, 58, 1, 58, 46, 39, 58, 1]--->target: [46]\n",
      "input: [25]--->target: [17]\n",
      "input: [25, 17]--->target: [27]\n",
      "input: [25, 17, 27]--->target: [10]\n",
      "input: [25, 17, 27, 10]--->target: [0]\n",
      "input: [25, 17, 27, 10, 0]--->target: [21]\n",
      "input: [25, 17, 27, 10, 0, 21]--->target: [1]\n",
      "input: [25, 17, 27, 10, 0, 21, 1]--->target: [54]\n",
      "input: [25, 17, 27, 10, 0, 21, 1, 54]--->target: [39]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch(split=\"train\")\n",
    "print(f\"inputs: {xb}\")\n",
    "print(f\"targets: {yb}\")\n",
    "\n",
    "for b in range(batch_size): #batch dimension\n",
    "    for t in range(block_size): #time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t:t+1]\n",
    "\n",
    "        print(f\"input: {context.tolist()}--->target: {target.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fac627-7bb5-4cac-86e2-93f0b5069d3c",
   "metadata": {},
   "source": [
    "### Creating layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f152e762-ca67-4482-8db4-b882a0ada7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 65])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self,ix, targets):\n",
    "        logits = self.embedding(ix)#B,T,C#batch, time, Channel(emb dim)\n",
    "        return logits\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "out = model(xb, yb)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e724a94a-3e63-4a2b-a529-ff9036842e28",
   "metadata": {},
   "source": [
    "#### Obtaining loss (wrong pytorch syntax way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb98dc1c-6fcd-4733-84e8-c75f84cf68fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 65], got [4, 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m logits\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m BigramLanguageModel(vocab_size)\n\u001b[0;32m---> 13\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m out\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, ix, targets)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,ix, targets):\n\u001b[1;32m      7\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(ix)\u001b[38;5;66;03m#B,T,C#batch, time, Channel(emb dim)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [4, 65], got [4, 8]"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self,ix, targets):\n",
    "        logits = self.embedding(ix)#B,T,C#batch, time, Channel(emb dim)\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "out = model(xb, yb)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66848d00-d195-4f4f-9c93-bf54d62e118e",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "\n",
    "Our dimension is B,T,C but torch expects B,C,T.\n",
    "\n",
    "we had better pass using the shape of B*T, C instead of B,T,C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "323d7a92-7d7e-4f5b-a54d-cca61fd5698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.4971, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self,ix, targets):\n",
    "        logits = self.embedding(ix)#B,T,C#batch, time, Channel(emb dim)\n",
    "\n",
    "        B,T,C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "out, loss = model(xb, yb)\n",
    "print(out.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e093a-04ec-45a5-99da-d239d6aed6bc",
   "metadata": {},
   "source": [
    "We have 65 vocab size. so probability for each to be randomly selected is 1/65. so random loss should be -log(1/65)=4.1743872698956. Our loss is larger than that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce930c-3d28-4e93-98c1-30b8293cb4db",
   "metadata": {},
   "source": [
    "#### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a91aec-766c-4d93-b465-59fce9ee3e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self,ix, targets=None):\n",
    "        logits = self.embedding(ix)#B,T,C#batch, time, Channel(emb dim)\n",
    "\n",
    "        if targets==None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_token):\n",
    "\n",
    "        for _ in range(max_new_token):\n",
    "            logits, _ = self(idx)\n",
    "            # print(logits.shape)\n",
    "            logits = logits[:, -1, :]\n",
    "            # print(logits.shape)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # print(idx_next.shape)\n",
    "            idx = torch.cat((idx, idx_next),dim=1)\n",
    "            \n",
    "            # print(idx.shape)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "out, loss = model(xb, yb)\n",
    "print(out.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45172cab-26c7-499c-a3c9-5838151a85a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 31, 56, 12, 55, 28,  7, 29, 35, 49, 58, 36, 53, 24,  4, 48, 24, 16,\n",
       "         22, 45, 27, 24, 34, 64,  5, 30, 21, 53, 16, 55, 20, 42, 46, 57, 34,  4,\n",
       "         60, 24, 24, 62, 39, 58, 48, 57, 41, 25, 54, 61, 24, 17, 30, 31, 28, 63,\n",
       "         39, 53,  8, 55, 44, 64, 57,  3, 37, 57,  3, 64, 18,  7, 61,  6, 11, 43,\n",
       "         17, 49, 64, 62, 48, 45, 15, 23, 18, 15, 46, 57,  2, 47, 35, 35,  8, 27,\n",
       "         40, 64, 16, 52, 62, 13,  1, 25, 57,  3,  9]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = model.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_token=100)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f374fc3e-a790-4282-98fb-3a5d328711fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 101])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape #B,max token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "705e6148-bbfd-4628-acd8-0eb98996b566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568a3bf-7030-48df-bb91-521befd588b8",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2becd7a2-6c2a-4bfd-87f6-f77b53e779fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04f89943-4c96-4fb9-b647-9426bd42dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "# optimizer = optimizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64c9b210-0fd0-496e-b0e9-73edc71e9e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.729414939880371\n",
      "2.608893394470215\n",
      "2.5848662853240967\n",
      "2.4988811016082764\n",
      "2.4509129524230957\n",
      "2.384185552597046\n",
      "2.4576575756073\n",
      "2.4058115482330322\n",
      "2.39339280128479\n",
      "2.5372300148010254\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for i in range(10000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    xb,yb = xb.to(device), yb.to(device)\n",
    "    logits, loss= model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%1000==0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84d7c9c4-f6b7-419f-b4bf-84c7e7d22f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 16, 33, 15, 39, 42, 53, 53, 52, 48, 59,  1, 41, 47, 43,  1, 57, 58,\n",
       "          1, 58,  1, 58, 56, 43, 43,  1, 21, 21, 27, 24, 17, 26, 30, 17, 32, 46,\n",
       "         39, 52, 42,  1, 58,  6,  1, 39, 45, 46, 39, 58, 46,  6,  0, 25, 21, 10,\n",
       "          1, 18, 53, 50,  5, 57, 58, 53, 56, 43,  1, 25, 21, 44,  1, 53, 57,  1,\n",
       "         58, 46, 53,  1, 45, 43, 39, 58, 11,  1, 58, 47, 57, 46, 43, 43, 56,  1,\n",
       "         50, 47, 42,  1, 39, 50,  5, 57, 58,  1, 40]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "idx = model.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_token=100)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa3aa882-c710-490f-8749-a1f6967cf905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDUCadoonju cie st t tree IIOLENREThand t, aghath,\\nMI: Fol'store MIf os tho geat; tisheer lid al'st b\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(idx[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3e058-aeb7-4bf3-a22b-92bdba50f8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
